---
title: "MATH 216 Homework 4"
author: "Joy Wood"
output: html_document
---

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# For data manipulation and visualization
#install.packages("mosaic")
suppressPackageStartupMessages(library(mosaic))
suppressPackageStartupMessages(library(stringr))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(tidyr))
suppressPackageStartupMessages(library(knitr))
# For US county and state maps
suppressPackageStartupMessages(library(maps))
# For loading in shapefiles
suppressPackageStartupMessages(library(rgdal))
suppressPackageStartupMessages(library(maptools))
# For interactive maps
suppressPackageStartupMessages(library(leaflet))
```

## Admistrative:

Please indicate

* Who you collaborated with: talked to Christian about data and leaflet
* Roughly how much time you spent on this HW: ~6 hours
* What gave you the most trouble:
* Any comments you have: 


## Question 1:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Load state and county map of US in 2010 from the maps package and convert them
# to data frames so that we can ggplot them.
US_state <- map_data("state") %>% 
  tbl_df()
US_county <- map_data("county") %>% 
  tbl_df()
county_data <- read.csv("data/COUNTY.csv", header=TRUE) %>% 
  tbl_df()
```

### Choropleth Map of US Voter Behavior in 2000
```{r, echo=FALSE, message=FALSE, warning=FALSE}
clean_text <- function(text){
  text <- gsub("[^[:alnum:]]", "", text)
  text <- gsub(" ", "", text)
  text <- tolower(str_trim(text))
  return(text)
}
#factor to num
county_data$PNADER <- as.numeric(as.character(county_data$PNADER ))
county_data$PGORE <- as.numeric(as.character(county_data$PGORE ))
county_data$PBUSH <- as.numeric(as.character(county_data$PBUSH))

#replace na with 0
county_data$PNADER[is.na(county_data$PNADER)] <- 0
county_data$PBUSH[is.na(county_data$PBUSH)] <- 0
county_data$PGORE[is.na(county_data$PGORE)] <- 0

#clean text
US_county$region <- clean_text(US_county$region)
US_county$subregion <- clean_text(US_county$subregion)
county_data$STATE <- clean_text(county_data$STATE)
county_data$COUNTY <- clean_text(county_data$COUNTY)
county_data$STATE <- clean_text(county_data$STATE)
county_data$COUNTY <- clean_text(county_data$COUNTY)

#aggregate democrats
county_data$PNADER_PGORE <- county_data$PGORE + county_data$PNADER

#categorical party var
county_geo_data<- left_join(US_county, county_data, by = c( "region" = "STATE", "subregion" = "COUNTY")) %>% 
  na.omit() %>% 
  mutate(party = derivedFactor(
    "D" = (PNADER_PGORE-PBUSH > .03),
    "R" = (PBUSH-PNADER_PGORE > .1),
    .method = "first",
    .default = "S"
    )) %>% 
  select(-PNADER, -PGORE)

#color scheme
myColors <- c("#1B338B","#8B1B33","#FFFFFF")
names(myColors) <- levels(county_geo_data$party)
colScale <- scale_fill_manual(name = "party",values = myColors, labels=c("Democrat", "Republican", "Split"))

#base map
g <- ggplot(data=NULL) +
geom_polygon(data=county_geo_data, aes(x=long, y=lat, group=group, fill=party)) +
geom_path(data=county_geo_data, aes(x=long, y=lat, group=group), col="grey", size=0.1) +
geom_path(data=US_state, aes(x=long, y=lat, group=group), col="black", size=0.2) +
coord_map() +
guides(fill=guide_legend(title=NULL))+
labs(title = "Prefered Party By County in the 2000 Election", x = "longitude", y = "latitude")+
  theme_map()+
  theme(legend.background = element_rect(fill="gray90", size=.5))

#add color
g+colScale

```


### Write-Up

```{r, fig.width=12, fig.height=6}

#Spatial Autocorrelation: Moran's I
county_geo_data
# Run the next bit of code a few times to convince yourselves of this fact.
county_geo_data.dists <- as.matrix(dist(cbind(county_geo_data$lon, county_geo_data$lat)))

ozone.dists.inv <- 1/ozone.dists
diag(ozone.dists.inv) <- 0
# We then compute Moran's I using Y_i = noise.  Look at the statistic and 
# p-value for multiple runs of the following two lines. Again, run this a few 
# times to see how the p-value behaves.
noise <- runif(n, min=0, max=1)
moran.test(noise, PDX_weights)


```



## Question 2:

### Loading Shapefile Data

Here is some starter code:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#I'm not sure why these don't/rarely knit, they compile fine for me

shapefile_name <- paste(getwd(), "/VT_census_tracts/tl_2015_50_tract.shp", sep="")
VT <- readOGR(shapefile_name, layer = "tl_2015_50_tract", verbose = FALSE)

#import, select, rename 
vt_census <- read.csv("data/VT_census_tract.csv", header=TRUE) %>% 
  tbl_df()

vt_census <- select(vt_census,  GEOID = Geo_FIPS, total_pop = SE_T055_001, white = SE_T055_003, black = SE_T055_004, hispanic = SE_T055_010, native = SE_T055_005, asian = SE_T055_006, islander = SE_T055_007, other = SE_T055_008, mix = SE_T055_009)


#calc proportions
vt_census <- vt_census %>% mutate(
  prop_white = white/total_pop,
  prop_black = black/total_pop,
  prop_hispanic = hispanic/total_pop,
  prop_native = native/total_pop, 
  prop_asian = asian/total_pop, 
  prop_islander = islander/total_pop, 
  prop_other = other/total_pop,
  prop_mix = mix/total_pop)


base <- leaflet(VT) %>%
  addProviderTiles("CartoDB.Positron") %>%
  addPolylines(data = VT, color="#FFFFFF", weight=2)

#hispanic
pal <- colorQuantile("YlOrRd", NULL, n = 5)
base %>% addPolygons(
              fillColor = ~pal(1 - vt_census$prop_hispanic),#inverts color scheme
              fillOpacity = 0.5, 
              weight = 1)
#white
pal <- colorQuantile("YlGnBu", NULL, n = 5)
base %>% addPolygons(
              fillColor = ~pal(1 - vt_census$prop_white),#inverts color scheme
              fillOpacity = 0.5, 
              weight = 1)
#black
pal <- colorQuantile("RdPu", NULL, n = 5)
base %>% addPolygons(
              fillColor = ~pal(1 - vt_census$prop_black),#inverts color scheme
              fillOpacity = 0.5, 
              weight = 1)
#asian
pal <- colorQuantile("Oranges", NULL, n = 5)
base %>% addPolygons(
              fillColor = ~pal(1 - vt_census$prop_black),#inverts color scheme
              fillOpacity = 0.5, 
              weight = 1)

```


### Write-Up
Comment on general ethnic demographic trends that's more substantive than just
"Vermont is really white."



